#include <torch/extension.h>
#include <pybind11/pybind11.h>
#include "acl/acl.h"
#include "tiling/platform/platform_ascendc.h"
// #include "register/tilingdata_base.h"
// #include "tiling/context/context_builder.h"
#include "aclnn/opdev/platform.h"
#include "data_utils.h"
#include "utils.h"
#include <torch/library.h>
#include <torch_npu/csrc/core/npu/NPUStream.h>
#include <torch_npu/csrc/framework/OpCommand.h>
#include <torch_npu/csrc/npu/Module.h>
#include "iostream"

// #include "op_plugin/OpApiInterface.h"

template <typename T>
struct TorchTypeToAscendType {
  using type = T;
  using ptr_type = T*;
};

turbo_types get_dtype_from_torch(at::ScalarType scalar_type) {
  if (scalar_type == at::ScalarType::Float) {
    return turbo_types::FP32;
  } else if (scalar_type == at::ScalarType::BFloat16) {
    return turbo_types::BF16;
  } else {
    return turbo_types::FP16;
  }
}



#define AT_DISPATCH_CASE(enum_type, ...) \
  AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)

#define TURBO_DISPATCH_CASE_FLOATING_TYPES(...) \
  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__) \
  AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)

#define TURBO_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...) \
    AT_DISPATCH_SWITCH(TYPE, NAME, TURBO_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))
extern void add_custom_do(uint32_t blockDim, void *stream, uint8_t *x, uint8_t *y, uint8_t *z);


extern void rope_custom_do(
    turbo_types type,
    bool is_neox,
    void* stream,
    int64_t* positions,
    void* query_dst,
    void* key_dst,
    void* query,
    void* key,
    void* cos_sin_cache,
    const int rot_dim,
    const int64_t query_stride,
    const int64_t key_stride,
    const int64_t dst_query_stride,
    const int64_t dst_key_stride,
    const int num_heads,
    const int num_kv_heads,
    const int head_size,
    const int64_t num_tokens,
    const uint32_t loop_cnt,
    const uint32_t aivNum);


extern void rope_custom_quant_do(
    turbo_types type,
    bool is_neox,
    void* stream,
    int64_t* positions,
    void* query_dst,
    void* key_dst,
    void* query,
    void* key,
    void* cos_sin_cache,
    void* q_scale,
    void* k_scale,
    void* q_offset,
    void* k_offset,
    const int rot_dim,
    const int64_t query_stride,
    const int64_t key_stride,
    const int64_t dst_query_stride,
    const int64_t dst_key_stride,
    const int num_heads,
    const int num_kv_heads,
    const int head_size,
    const int64_t num_tokens,
    const uint32_t loop_cnt,
    uint32_t aivNum);

std::tuple<at::Tensor, at::Tensor> rope_torch(
    at::Tensor& positions,
    at::Tensor& query,
    at::Tensor& key,
    at::Tensor& cos_sin_cache,
    int64_t head_size,
    bool is_neox
) {
    // CHECK_ACL(aclInit(nullptr));
    int32_t deviceId = 0;
    int64_t num_tokens = query.numel() / query.size(-1);;
    // std::cout << " num tokens equals to: " << num_tokens << std::endl;
    int rot_dim = cos_sin_cache.size(1);
    int num_heads = query.size(-1) / head_size;
    int num_kv_heads = key.size(-1) / head_size;
    int64_t* position_ids_ptr = positions.data_ptr<int64_t>();
    void* query_ptr = query.data_ptr();
    void* key_ptr = key.data_ptr();
    void* cos_sin_cache_ptr = cos_sin_cache.data_ptr();
    // int64_t parallel_cnt = num_tokens * num_kv_heads;
    at::Tensor query_dst = at::empty({num_tokens, num_heads, head_size}, query.options());
    at::Tensor key_dst = at::empty({num_tokens, num_kv_heads, head_size}, key.options());
    void* query_dst_ptr = query_dst.data_ptr();
    void* key_dst_ptr = key_dst.data_ptr();
    int64_t query_stride = query.stride(-2);
    int64_t key_stride = key.stride(-2);
    int64_t dst_query_stride = query_dst.stride(0);
    int64_t dst_key_stride = key_dst.stride(0);
    at::ScalarType scalar_type = query.scalar_type();
    aclrtStream stream = c10_npu::getCurrentNPUStream().stream();
    at_npu::native::OpCommand cmd;
    cmd.Name("turbo_rope");
    cmd.SetCustomHandler([
      scalar_type,
      is_neox,
      num_tokens,
      stream,
      position_ids_ptr,
      query_dst_ptr,
      key_dst_ptr,
      query_ptr,
      key_ptr,
      cos_sin_cache_ptr,
      rot_dim,
      query_stride,
      key_stride,
      dst_query_stride,
      dst_key_stride,
      num_heads,
      num_kv_heads,
      head_size
    ]() -> int {
        turbo_types dtype_num = get_dtype_from_torch(scalar_type);
        // auto tilingContextHolder = context_ascendc::ContextBuilder().BuildTilingContext();
        // auto platform_manager = ;
        fe::PlatFormInfos platform_infos;
        int device_id = 0;
        // fe::OptionalInfo optional;
        fe::PlatformInfoManager::GeInstance().GetRuntimePlatformInfosByDevice(device_id, platform_infos);
        // auto platform_info = op::GetCurrentPlatformInfo();
        // gert::TilingContext *context = tilingContextHolder->GetContext<gert::TilingContext>();
        auto ascendcPlatform = platform_ascendc::PlatformAscendC(&platform_infos);
        uint32_t aivNum = ascendcPlatform.GetCoreNumAiv();
        uint32_t loop_cnt = (num_tokens + aivNum - 1) / aivNum;
        // auto aivNum = platform_info.GetVectorCoreNum();
        rope_custom_do(
          dtype_num,
          is_neox,
          stream,
          position_ids_ptr,
          query_dst_ptr,
          key_dst_ptr,
          query_ptr,
          key_ptr,
          cos_sin_cache_ptr,
          rot_dim,
          query_stride,
          key_stride,
          dst_query_stride,
          dst_key_stride,
          num_heads,
          num_kv_heads,
          head_size,
          num_tokens,
          loop_cnt,
          aivNum
        );
        return 0;
    });
    cmd.Run();
    return {query_dst, key_dst};
}

std::tuple<at::Tensor, at::Tensor> rope_quant_torch(
    at::Tensor& positions,
    at::Tensor& query,
    at::Tensor& key,
    at::Tensor& cos_sin_cache,
    at::Tensor& q_scale,
    at::Tensor& k_scale,
    at::Tensor& q_offset,
    at::Tensor& k_offset,
    int64_t head_size,
    bool is_neox
) {
    // CHECK_ACL(aclInit(nullptr));
    int32_t deviceId = 0;
    int64_t num_tokens = query.numel() / query.size(-1);;
    // std::cout << " num tokens equals to: " << num_tokens << std::endl;
    int rot_dim = cos_sin_cache.size(1);
    int num_heads = query.size(-1) / head_size;
    int num_kv_heads = key.size(-1) / head_size;
    int64_t* position_ids_ptr = positions.data_ptr<int64_t>();
    void* query_ptr = query.data_ptr();
    void* key_ptr = key.data_ptr();
    void* cos_sin_cache_ptr = cos_sin_cache.data_ptr();
    // int64_t parallel_cnt = num_tokens * num_kv_heads;
    at::Tensor query_dst = at::empty({num_tokens, num_heads, head_size}, query.options().dtype(q_offset.scalar_type()));
    at::Tensor key_dst = at::empty({num_tokens, num_kv_heads, head_size}, key.options().dtype(k_offset.scalar_type()));
    void* query_dst_ptr = query_dst.data_ptr();
    void* key_dst_ptr = key_dst.data_ptr();
    void* q_scale_ptr = q_scale.data_ptr();
    void* k_scale_ptr = k_scale.data_ptr();
    void* q_offset_ptr = q_offset.data_ptr();
    void* k_offset_ptr = k_offset.data_ptr();
    int64_t query_stride = query.stride(-2);
    int64_t key_stride = key.stride(-2);
    int64_t dst_query_stride = query_dst.stride(0);
    int64_t dst_key_stride = key_dst.stride(0);
    at::ScalarType scalar_type = query.scalar_type();
    aclrtStream stream = c10_npu::getCurrentNPUStream().stream();
    at_npu::native::OpCommand cmd;
    cmd.Name("turbo_rope_quant");
    cmd.SetCustomHandler([
      scalar_type,
      is_neox,
      num_tokens,
      stream,
      position_ids_ptr,
      query_dst_ptr,
      key_dst_ptr,
      query_ptr,
      key_ptr,
      cos_sin_cache_ptr,
      q_scale_ptr,
      k_scale_ptr,
      q_offset_ptr,
      k_offset_ptr,
      rot_dim,
      query_stride,
      key_stride,
      dst_query_stride,
      dst_key_stride,
      num_heads,
      num_kv_heads,
      head_size
    ]() -> int {
        turbo_types dtype_num = get_dtype_from_torch(scalar_type);
        // auto tilingContextHolder = context_ascendc::ContextBuilder().BuildTilingContext();
        // auto platform_manager = ;
        fe::PlatFormInfos platform_infos;
        int device_id = 0;
        // fe::OptionalInfo optional;
        fe::PlatformInfoManager::GeInstance().GetRuntimePlatformInfosByDevice(device_id, platform_infos);
        // auto platform_info = op::GetCurrentPlatformInfo();
        // gert::TilingContext *context = tilingContextHolder->GetContext<gert::TilingContext>();
        auto ascendcPlatform = platform_ascendc::PlatformAscendC(&platform_infos);
        uint32_t aivNum = ascendcPlatform.GetCoreNumAiv();
        uint32_t loop_cnt = (num_tokens + aivNum - 1) / aivNum;
        // auto aivNum = platform_info.GetVectorCoreNum();
        rope_custom_quant_do(
          dtype_num,
          is_neox,
          stream,
          position_ids_ptr,
          query_dst_ptr,
          key_dst_ptr,
          query_ptr,
          key_ptr,
          cos_sin_cache_ptr,
          q_scale_ptr,
          k_scale_ptr,
          q_offset_ptr,
          k_offset_ptr,
          rot_dim,
          query_stride,
          key_stride,
          dst_query_stride,
          dst_key_stride,
          num_heads,
          num_kv_heads,
          head_size,
          num_tokens,
          loop_cnt,
          aivNum
        );
        return 0;
    });
    cmd.Run();
    return {query_dst, key_dst};
}

void run_add(at::Tensor& a, at::Tensor& b, at::Tensor& c) {
    uint32_t blockDim = 8;
    size_t inputByteSize = 8 * 2048 * sizeof(uint16_t);
    size_t outputByteSize = 8 * 2048 * sizeof(uint16_t);
    int32_t deviceId = 0;
    CHECK_ACL(aclrtSetDevice(deviceId));
    aclrtStream stream = nullptr;
    CHECK_ACL(aclrtCreateStream(&stream));

    uint8_t *xHost, *yHost, *zHost;
    uint8_t *xDevice, *yDevice, *zDevice;

    CHECK_ACL(aclrtMallocHost((void **)(&xHost), inputByteSize));
    CHECK_ACL(aclrtMallocHost((void **)(&yHost), inputByteSize));
    CHECK_ACL(aclrtMallocHost((void **)(&zHost), outputByteSize));
    CHECK_ACL(aclrtMalloc((void **)&xDevice, inputByteSize, ACL_MEM_MALLOC_HUGE_FIRST));
    CHECK_ACL(aclrtMalloc((void **)&yDevice, inputByteSize, ACL_MEM_MALLOC_HUGE_FIRST));
    CHECK_ACL(aclrtMalloc((void **)&zDevice, outputByteSize, ACL_MEM_MALLOC_HUGE_FIRST));

    ReadFile("../input/input_x.bin", inputByteSize, xHost, inputByteSize);
    ReadFile("../input/input_y.bin", inputByteSize, yHost, inputByteSize);

    CHECK_ACL(aclrtMemcpy(xDevice, inputByteSize, xHost, inputByteSize, ACL_MEMCPY_HOST_TO_DEVICE));
    CHECK_ACL(aclrtMemcpy(yDevice, inputByteSize, yHost, inputByteSize, ACL_MEMCPY_HOST_TO_DEVICE));
    uint8_t * x_device = reinterpret_cast<uint8_t*>(a.data_ptr());

    add_custom_do(blockDim, stream, x_device, yDevice, zDevice);
    CHECK_ACL(aclrtSynchronizeStream(stream));

    CHECK_ACL(aclrtMemcpy(zHost, outputByteSize, zDevice, outputByteSize, ACL_MEMCPY_DEVICE_TO_HOST));
    WriteFile("../output/output_z.bin", zHost, outputByteSize);

    CHECK_ACL(aclrtFree(xDevice));
    CHECK_ACL(aclrtFree(yDevice));
    CHECK_ACL(aclrtFree(zDevice));
    CHECK_ACL(aclrtFreeHost(xHost));
    CHECK_ACL(aclrtFreeHost(yHost));
    CHECK_ACL(aclrtFreeHost(zHost));

    CHECK_ACL(aclrtDestroyStream(stream));
    CHECK_ACL(aclrtResetDevice(deviceId));
    // CHECK_ACL(aclFinalize());
}
void test_torch(at::Tensor& positions) {
  return ;
}


// TORCH_LIBRARY_FRAGMENT(turbo, m) {
//   m.def("rope_torch(Tensor! pos, Tensor! query, Tensor! key, Tensor! value, int head_size, bool is_neox)->()");
//   m.impl("rope_torch", torch::kNPU, &rope_torch);
// }



PYBIND11_MODULE(my_module, m) {
    m.def("rope_torch", &rope_torch, "rope_torch");
    m.def("rope_quant_torch", &rope_quant_torch, "rope_quant_torch");
    m.def("test_torch", &test_torch, "test_torch");
    m.def("run_add", &run_add, "run_add");
}